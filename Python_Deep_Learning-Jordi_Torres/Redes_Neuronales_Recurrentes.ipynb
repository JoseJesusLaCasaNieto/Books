{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Recurrentes (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vectorización de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['me' 'gusta' 'el' 'deep' 'learning']\n",
      "[4 2 1 0 3]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pepel\\.conda\\envs\\PySpace310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "doc = \"Me gusta el Deep Learning\"\n",
    "\n",
    "doc = doc.lower()\n",
    "doc = doc.split()\n",
    "values = array(doc)\n",
    "print(values)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Embedding layer de Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Programando una RNN: Generación de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Descarga y preprocesado de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del texto: 203251 caráceters\n",
      "El texto está compuesto de estos 92 caráceteres:\n",
      "\n",
      "['\\n', '\\r', ' ', '!', '\"', '#', '%', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xad', 'ÿ', 'Š', '‡', '…']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path_txt = os.path.join(\"DeepLearning-Introduccion-practica-con-Keras-PRIMERA-PARTE.txt\")\n",
    "\n",
    "text = open(path_txt, 'rb').read().decode(encoding='utf-8')\n",
    "print(\"Longitud del texto: {} caráceters\".format(len(text)))\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "print(\"El texto está compuesto de estos {} caráceteres:\".format(len(vocab)))\n",
    "print(f\"\\n{vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "char2dix = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " '\\n':   0,\n",
      " '\\r':   1,\n",
      " ' ' :   2,\n",
      " '!' :   3,\n",
      " '\"' :   4,\n",
      " '#' :   5,\n",
      " '%' :   6,\n",
      " \"'\" :   7,\n",
      " '(' :   8,\n",
      " ')' :   9,\n",
      " '*' :  10,\n",
      " '+' :  11,\n",
      " ',' :  12,\n",
      " '-' :  13,\n",
      " '.' :  14,\n",
      " '/' :  15,\n",
      " '0' :  16,\n",
      " '1' :  17,\n",
      " '2' :  18,\n",
      " '3' :  19,\n",
      " '4' :  20,\n",
      " '5' :  21,\n",
      " '6' :  22,\n",
      " '7' :  23,\n",
      " '8' :  24,\n",
      " '9' :  25,\n",
      " ':' :  26,\n",
      " ';' :  27,\n",
      " '<' :  28,\n",
      " '=' :  29,\n",
      " '>' :  30,\n",
      " '?' :  31,\n",
      " '@' :  32,\n",
      " 'A' :  33,\n",
      " 'B' :  34,\n",
      " 'C' :  35,\n",
      " 'D' :  36,\n",
      " 'E' :  37,\n",
      " 'F' :  38,\n",
      " 'G' :  39,\n",
      " 'H' :  40,\n",
      " 'I' :  41,\n",
      " 'J' :  42,\n",
      " 'K' :  43,\n",
      " 'L' :  44,\n",
      " 'M' :  45,\n",
      " 'N' :  46,\n",
      " 'O' :  47,\n",
      " 'P' :  48,\n",
      " 'Q' :  49,\n",
      " 'R' :  50,\n",
      " 'S' :  51,\n",
      " 'T' :  52,\n",
      " 'U' :  53,\n",
      " 'V' :  54,\n",
      " 'W' :  55,\n",
      " 'X' :  56,\n",
      " 'Y' :  57,\n",
      " '[' :  58,\n",
      " ']' :  59,\n",
      " '_' :  60,\n",
      " 'a' :  61,\n",
      " 'b' :  62,\n",
      " 'c' :  63,\n",
      " 'd' :  64,\n",
      " 'e' :  65,\n",
      " 'f' :  66,\n",
      " 'g' :  67,\n",
      " 'h' :  68,\n",
      " 'i' :  69,\n",
      " 'j' :  70,\n",
      " 'k' :  71,\n",
      " 'l' :  72,\n",
      " 'm' :  73,\n",
      " 'n' :  74,\n",
      " 'o' :  75,\n",
      " 'p' :  76,\n",
      " 'q' :  77,\n",
      " 'r' :  78,\n",
      " 's' :  79,\n",
      " 't' :  80,\n",
      " 'u' :  81,\n",
      " 'v' :  82,\n",
      " 'w' :  83,\n",
      " 'x' :  84,\n",
      " 'y' :  85,\n",
      " 'z' :  86,\n",
      " '\\xad':  87,\n",
      " 'ÿ' :  88,\n",
      " 'Š' :  89,\n",
      " '‡' :  90,\n",
      " '…' :  91,\n"
     ]
    }
   ],
   "source": [
    "for char, _ in zip(char2dix, range(len(vocab))):\n",
    "    print(\" {:4s}: {:3d},\".format(repr(char), char2dix[char]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2dix[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texto: 'Prologo\\r\\nEn 1953, Isaac Asimov publico Segunda Fun'\n",
      "array([48, 78, 75, 72, 75, 67, 75,  1,  0, 37, 74,  2, 17, 25, 21, 19, 12,\n",
      "        2, 41, 79, 61, 61, 63,  2, 33, 79, 69, 73, 75, 82,  2, 76, 81, 62,\n",
      "       72, 69, 63, 75,  2, 51, 65, 67, 81, 74, 64, 61,  2, 38, 81, 74])\n"
     ]
    }
   ],
   "source": [
    "print(\"texto: {}\".format(repr(text[:50])))\n",
    "print(\"{}\".format(repr(text_as_int[:50])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Preparación de los datos para ser usados por la RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "seq_length = 100\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Prologo\\r\\nEn 1953, Isaac Asimov publico Segunda Fundacion, el tercer libro de la saga de la Fundacion '\n",
      "'(o el decimotercero segun otras fuentes, este es un tema de debate). En Segunda Fundacion aparece por'\n",
      "' primera vez Arkady Darell, uno de los principales personajes de la parte final de la saga. En su pri'\n",
      "'mera escena, Arkady, que tiene 14 anos, esta haciendo sus tareas escolares. En concreto, una redaccio'\n",
      "'n que lleva por titulo ?El Futuro del Plan Sheldon?. Para hacer la redaccion, Arkady esta utilizando '\n",
      "'un ?transcriptor?,un dispositivo que convierte su voz en palabras escritas. Este tipo de dispositivo,'\n",
      "' que para Isaac Asimov era ciencia ficcion en 1953, lo tenemos al alcance de la mano en la mayoria de'\n",
      "' nuestros smartphones, y el Deep Learning es uno de los responsables de que ya tengamos este tipo de '\n",
      "'aplicaciones, siendo la tecnologia otro de ellos.En la actualidad disponemos de GPUs (Graphics Proces'\n",
      "'sor Units), que solo cuestan alrededor de 100 euros, que estarian en la lista del Top500 hace unos po'\n"
     ]
    }
   ],
   "source": [
    "for item in sequences.take(10):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Prologo\\r\\nEn 1953, Isaac Asimov publico Segunda Fundacion, el tercer libro de la saga de la Fundacion'\n",
      "Target data:  'rologo\\r\\nEn 1953, Isaac Asimov publico Segunda Fundacion, el tercer libro de la saga de la Fundacion '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input data: \", repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print(\"Target data: \", repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int32, name=None), TensorSpec(shape=(64, 100), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Construcción del modelo RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        batch_input_shape=[batch_size, None]\n",
    "    ))\n",
    "    model.add(LSTM(\n",
    "        rnn_units,\n",
    "        return_sequences=True,\n",
    "        stateful=True,\n",
    "        recurrent_initializer='glorot_uniform'\n",
    "    ))\n",
    "    model.add(Dense(vocab_size))\n",
    "    return model\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           23552     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 92)            94300     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,364,828\n",
      "Trainable params: 5,364,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (64, 100) # (batch_size, sequence_length)\n",
      "Target: (64, 100) # (batch_size, sequence_length)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(\"Input:\", input_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
    "    print(\"Target:\", target_example_batch.shape, \"# (batch_size, sequence_length)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  (64, 100, 92) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(\"Prediction: \", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(\n",
    "    example_batch_predictions[0],\n",
    "    num_samples=1)\n",
    "\n",
    "sampled_indices_characters = tf.squeeze(\n",
    "    sampled_indices,\n",
    "    axis=-1\n",
    ").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21 50  7 66  0  3 55 64 14 71 79 17 61 78 35 14 61 16 10  2 58 71 55 62\n",
      " 74 40  7 43 23 10 29 55 41 12 60 57 23 74  5 90 26 71 61 26 24 53 88 81\n",
      " 16 34 76 29 11  0 43 44 15 50 91 47 60 38 72 25 68 14 55 47  4 21 64 85\n",
      " 70  6 38 28  7 69 61 91 90 31 91 28 26 61 13 23 47 53 56 60  8 65 90 49\n",
      " 12 56 21 54]\n"
     ]
    }
   ],
   "source": [
    "print(sampled_indices_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Entrenamiento del modelo RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels,\n",
    "        logits,\n",
    "        from_logits=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"cpkt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "31/31 [==============================] - 2s 29ms/step - loss: 3.2831\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 2.8602\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 2.5385\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 2.3278\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 2.1978\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 2.1068\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 2.0137\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 1.9147\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 1.8174\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 1.7198\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 1.6319\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 1.5492\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 1.4726\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 1.4042\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 1.3468\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 1.2928\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 1.2466\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 1.2058\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 1.1658\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 1.1280\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 1.0961\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 1.0621\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 1.0303\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 1.0022\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.9748\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.9476\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.9198\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.8971\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.8688\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.8410\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.8197\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.7925\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.7655\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.7405\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.7181\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.6931\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.6671\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.6433\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.6211\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.5985\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.5761\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.5513\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.5304\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.5087\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.4874\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.4698\n",
      "Epoch 47/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.4492\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.4354\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.4137\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.3995\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Generación de texto usando el modelo RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    rnn_units,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    num_generate = 500\n",
    "    input_eval = [char2dix[s] for s in start_string]\n",
    "\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    temperature = 0.5\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(\n",
    "            predictions,\n",
    "            num_samples=1\n",
    "        )[-1, 0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domingo que contiene el trazo de neuronas de la capa oculta con las GPU en un nuevo elemento X1 y por un solo el minimo global de la inteligencia artificial en Keras los actualmente este es la que obtiene en una nueva fuente de creacion de validacion introducida variables de entrada (features) utilizando una medida de la inteligencia artificial y la funcion de activacion softmax\n",
      "Vamos a resolver el problema de optimizacion sobre el tema del conjunto de datos de tener el proceso de aprendizaje de una re\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"Domingo \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activacion permite conectar tambien GPU y ed netro los sesgos que se consigue con el tamano de la segunda parte del libro.\n",
      "Precarga el ?pooling tiende se muestra los conocidos en cada epochs se permitir en los siguientes capitulos que tenemos un modelo que puede ser de ayuda al lector para ver como se codigo que presentaremos en el tema y mas complejo. \n",
      "Pero en realidad el calculo de parametros para los casos en cada iteracion, una vez aprendizaje con la accuracy de los datos de entrenamiento poco estan \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"Activacion \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redes nuestro modelo de referencia se ejecuta como se comporta con el que se esta es usar un grismo mis magiente puede definir en la imagen del docker.\n",
      "Para este articulo de referencia del capitulo anterior y permite reducir valores diendo los puntos anarieras en el ano 201251, corresponde a la capa de entrada (input layer), una o mas capas convolucionales que pueden ser aprenden este ejemplo simple, intentando como el supercomputador Marenostrum que se ajustara nos encontramos ante un nuevo valor de\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"Redes \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an la capa de neuronas. Mis a partir de la humanidad es comparable con la loss de cada capa oculta que contribuyen directamente a continuacion. \n",
      "En el proceso de entrenamiento contendra una capa convolucional su primer ejemplo de clasificacion que sigue al modelo y cada uno de los que ya hay disponible en dia es que en general, los de validacion con la imagen de entrada y de la prediccion de la red neuronal convolucional completa con la inteligencia artificial en general, y en cada capitulo. \n",
      "E\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"a\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpace310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
